{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PSTAT 234 Group Project: Hate Crime Data\n",
    "\n",
    "__PSTAT 234 Group Project members: Wenlu Gou, Nhan Huynh, Zach Terner, and Laura Urbisci__  \n",
    "__Presentation date: June 14th from 12:00 - 3:00 pm__\n",
    "\n",
    "___\n",
    "\n",
    "##Project overview:\n",
    "\n",
    "For the PSTAT 234 Final Group Project, we analyzed the hate crimes data set found in the [FiveThirtyEight Github repositiory](https://github.com/fivethirtyeight/data/tree/master/hate-crimes). The data set contains reported hate crime statistics in the United States broken up by state as well as demographic information for five years before and ten days after Trump was elected President. Bash and Python were used for the analysis.  \n",
    "\n",
    "\n",
    "The Jupyter notebook for this project is broken into a five sections: \n",
    "- Extracting the data\n",
    "- Pre-analysis and visualization\n",
    "- Analyzing the data\n",
    "- Model building \n",
    "- Conclusion\n",
    "\n",
    "The Jupyter notebook will be converted to slides using the steps outlined here (https://medium.com/@mjspeck/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extracting the data \n",
    "\n",
    "This section of the notebook contains the packages needed to run all of the code in this notebook in addition to the code used for data extraction.  \n",
    "\n",
    "\n",
    "We obtained data from the following sources:\n",
    "1. The main data is based on a .csv file found in [FiveThirtyEight Github repositiory on hate crimes](https://github.com/fivethirtyeight/data/tree/master/hate-crimes)\n",
    "2. We scraped a table on [Wikipedia](https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_religiosity) based on Pew Research Center's study to identify the overall religious status.\n",
    "3. We scraped a table from an article published by the [Denver Post](https://www.denverpost.com/2015/10/08/chart-compare-the-average-age-in-each-u-s-state-2005-2014/) to obtain median age for each state. \n",
    "4. We also looked into how the country was divided during the Civil War to identify which states can be classfied as conservative  (**Note:** 2, 3, and 4 are the additional covariates we'd like add into the main data frame in 1). \n",
    "\n",
    "We first install  and import all the important packages for the analysis.\n",
    "### Installation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "! pip install lxml"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# install plotly package\n",
    "! pip install plotly"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "! python -c \"import plotly; plotly.tools.set_credentials_file(username='nhanhuynh', api_key='Y6hY7vdVGfrm6WJztBi3')\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "! pip install pydotplus"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "! dot -Tps tree.dot -o tree.ps "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!apt-get install libqtscript4-svg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install graphviz"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!apt-get install graphviz"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Import installed packages"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# importing all of the necessary python packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.cluster import KMeans\n",
    "from copy import deepcopy\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from numpy.random import randn\n",
    "import collections\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale \n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import graphviz as g"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Download data "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "wget https://raw.githubusercontent.com/fivethirtyeight/data/master/hate-crimes/hate_crimes.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "! head hate_crimes.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes = pd.read_csv(\"hate_crimes.csv\")\n",
    "crimes.info()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data scraping \n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = requests.get(\"https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_religiosity\")\n",
    "soup = BeautifulSoup(res.content,\"html5lib\") # import content of the webpage in html \n",
    "table = soup.find_all('table')[1] # find the second table\n",
    "df = pd.read_html(str(table))[0]\n",
    "df = df.drop(df.index[[0,30,53,54,55,56]]).iloc[:,[0,4]] # delete records that are U.S. territories and select only first and last column\n",
    "df.columns = [\"state\",\"percent_religious\"]\n",
    "df.percent_religious = df.percent_religious.str[-3:].str[0:2] # clean up the format\n",
    "df.percent_religious = df. percent_religious.astype(float)\n",
    "df.percent_religious = df. percent_religious/100\n",
    "df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check if all states in df are the same and in crimes:\n",
    "stateDF = df.state.unique()\n",
    "stateDF.sort()\n",
    "stateCr = crimes.state.unique()\n",
    "stateCr.sort()\n",
    "stateDF == stateCr"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# merge df into crimes using state column:\n",
    "crimes = pd.merge(crimes,df,on=[\"state\"],how=\"inner\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List of states and their groups during the Civil War:\n",
    "union = [\"Maine\",\"New York\",\"New Hampshire\",\"Vermont\",\"Massachusetts\",\"Connecticut\",\"Rhode Island\",\"Pennsylvania\",\"New Jersey\",\"Ohio\", \"Indiana\", \"Illinois\", \"Kansas\", \"Michigan\", \"Wisconsin\", \"Minnesota\", \"Iowa\", \"California\", \"Nevada\", \"Oregon\"]\n",
    "confederacy = [\"Texas\", \"Arkansas\", \"Louisiana\", \"Tennessee\", \"Mississippi\", \"Alabama\", \"Georgia\", \"Florida\", \"South Carolina\", \"North Carolina\", \"Virginia\"]\n",
    "\n",
    "\n",
    "def civilWar(state):\n",
    "  if state in union:\n",
    "    return 'union'\n",
    "  elif state in confederacy:\n",
    "    return 'confe'\n",
    "  else: return 'none'\n",
    "  \n",
    "\n",
    "# add the civilWar column:\n",
    "crimes['civilWar'] = crimes['state'].apply(civilWar)\n",
    "crimes.civilWar.unique()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert civilWar column to categorical variable\n",
    "crimes.civilWar = crimes.civilWar.astype('category')\n",
    "crimes.civilWar.value_counts()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "age = requests.get(\"https://www.denverpost.com/2015/10/08/chart-compare-the-average-age-in-each-u-s-state-2005-2014/\")\n",
    "soup = BeautifulSoup(age.content, \"lxml\") \n",
    "table = soup.find_all('table')[0]\n",
    "age = pd.read_html(str(table))[0].iloc[:,[0,2]]\n",
    "age.columns =['state','medianAge']\n",
    "age.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check if all states in age df are the same and in crimes:\n",
    "stateAge = age.state.unique()\n",
    "stateAge.sort()\n",
    "stateAge == stateCr"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# merge Age df into Crimes df using state column:\n",
    "crimes = pd.merge(crimes,age,on=[\"state\"],how=\"inner\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Translate US states to two letter codes (in order to create US map in Plotly package)\n",
    "postal = requests.get(\"https://www.infoplease.com/state-abbreviations-and-state-postal-codes\")\n",
    "soup = BeautifulSoup(postal.content, \"lxml\") \n",
    "table = soup.find_all('table')[0]\n",
    "postal = pd.read_html(str(table))[0].iloc[:,[0,2]]\n",
    "postal.columns = [\"state\",\"abbr\"]\n",
    "# merge postal df into crimes df using state column:\n",
    "crimes = pd.merge(crimes,postal,on=[\"state\"],how=\"inner\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "postal.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-analysis and visualization\n",
    "\n",
    "### Data cleaning\n",
    "In this study, we want to compare the hate crimes in two periods: (a) from 2010 - 2015 and (b) the first 10 days after Trump was elected President in the 58th quadrennial American presidential election. The statistics on hate crimes from these two periods are recorded in different scales as the unit in pre-election hate crimes is annually average (per 100,000 population) from 2010 to 2015 while post-election hate crimes is the 10-day average (per 100,000 population) from November 9 -18, 2016. We convert the pre-election group to the 10-day average unit before proceeding the analysis.  \n",
    "<br>\n",
    "In addition, there are three columns that have missing data. Observations are missing for the field that describes the share of the population that are not U.S. citizens in 2015 (`share_non_citizen`), hate crimes per 100,000 population from Nov. 9-18, 2016 (`hate_crimes_per_100k_splc`), and average annual hate crimes per 100,000 population in 2010-2015 (`avg_hatecrimes_per_100k_fbi`).\n",
    "\n",
    "#### Fill missing values in the predictors\n",
    "\n",
    "We observe that the column `share_non_citizen` has some missing values. We will impute these missing values based on an external source published on [Henry J Kaiser Family Foundation](https://www.kff.org/other/state-indicator/distribution-by-citizenship-status/?currentTimeframe=0&selectedRows=%7B%22states%22:%7B%22mississippi%22:%7B%7D%7D%7D&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D). Note: the time frame of the external table on population distribution by citizenship status in each state is in 2016 while `share_non_citizen` is recorded in 2015. According to the table, all three states (Maine, Mississippi, and South Dakota) with missing values have 1% of share of the population that are not U.S. citizens in 2016.  \n",
    "<br>\n",
    "We also compare the mean of share of the population that are not U.S. citizens between before and after filling the missing values , the difference is not significant. \n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes[\"fbi_standardized_pre\"] = crimes[\"avg_hatecrimes_per_100k_fbi\"]/36.525\n",
    "\n",
    "crimes.info() # structure of data - we see there is missing data in share non citizen, hate crimes per 100K, ave hate crimes, and therefore fbi standardized"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes[['fbi_standardized_pre','hate_crimes_per_100k_splc']].describe()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We observe the presence of missing values in the data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# number of missing values in pre-election group\n",
    "crimes['fbi_standardized_pre'].isnull().sum()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# number of missing values in post-election\n",
    "crimes['hate_crimes_per_100k_splc'].isnull().sum()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create a data set that doesn't contain missing values in these groups and examine the density functions:\n",
    "subCrimes = crimes[['fbi_standardized_pre','hate_crimes_per_100k_splc']].dropna() \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "sns.kdeplot(subCrimes['fbi_standardized_pre'],shade=True,label='Pre-election',color='b',bw=.1,alpha=0.5)\n",
    "sns.kdeplot(subCrimes['hate_crimes_per_100k_splc'],shade=True,label='Post-election',color='r',bw=.1,alpha=.5)\n",
    "ax.set(xlabel='Hate Crimes', ylabel='Density Function')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trace0 = go.Box(x=subCrimes['fbi_standardized_pre'],name = 'Pre-election',boxpoints='all',jitter=0.5,pointpos=-5,boxmean=True)\n",
    "trace1 = go.Box(x=subCrimes['hate_crimes_per_100k_splc'],name = 'Post-election',boxpoints='all',jitter=0.5,pointpos=-5,boxmean=True)\n",
    "\n",
    "data = [trace0, trace1]\n",
    "py.iplot(data)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Paired-test\n",
    "We ran a paired t-test to see if the number of hate crimes before the election is different than after the election. We found using a level of significance of  $\\alpha =5\\%$ that there is a significant between the number of hate crimes when Trump was elected. \n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Do the matched pairs t-test (can add it later)\n",
    "sp.stats.ttest_rel(subCrimes['fbi_standardized_pre'],subCrimes['hate_crimes_per_100k_splc'])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Clustering - to fill in missing values in the predictors\n",
    "We used clustering to impute the missing values in the response variables. \n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes = crimes.set_index('state') # set state column as row index\n",
    "# run this code once"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes[crimes['share_non_citizen'].isnull()].share_non_citizen"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# mean of share_non_citizen before filling missing values\n",
    "crimes.share_non_citizen.mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# share_non_citizen has 3 missing values (NaN) in Main, Mississippi, and South Dakota. \n",
    "# Replace these missing values with 1% from the Kaiser Family Foundation:\n",
    "crimes.loc[['Maine','Mississippi','South Dakota'],'share_non_citizen'] = .01\n",
    "crimes.share_non_citizen.mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualization\n",
    "We use a variety of plots to visualize the data (i.e. pair plots, correlation plot, density plots\n",
    "religous percentages by state plot, and boxplots). \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Transform the data from wide format to long format, then drop row IDs with missing values:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colsToDrop = ['avg_hatecrimes_per_100k_fbi'] # remove some columns out \n",
    "crimesL = crimes.drop(colsToDrop, axis=1)\n",
    "# renames some columns \n",
    "crimesL = crimesL.rename(columns={'hate_crimes_per_100k_splc': 'post', 'fbi_standardized_pre': 'pre'})\n",
    "\n",
    "# convert crimes from wide format to long format:\n",
    "crimesL = pd.melt(crimesL,id_vars=['median_household_income','share_unemployed_seasonal','share_population_in_metro_areas','share_population_with_high_school_degree','share_non_citizen','share_white_poverty', 'gini_index', 'share_non_white','share_voters_voted_trump','percent_religious', 'civilWar', 'medianAge','abbr'],var_name='time', value_name='hateCrimes')\n",
    "crimesL = crimesL.dropna()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To understand the relationship between `share_voters_voted_trump` and `share_population_with_high_school_degree` on `hateCrimes` before and after the election, we can create a bubble plot as follows:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "post = crimesL[crimesL.time==\"post\"].loc[:,[\"share_voters_voted_trump\",\"share_population_with_high_school_degree\",\"hateCrimes\",\"abbr\"]]\n",
    "post['abbr'] = post['abbr'].astype(str)\n",
    "pre = crimesL[crimesL.time==\"pre\"].loc[:,[\"share_voters_voted_trump\",\"share_population_with_high_school_degree\",\"hateCrimes\",\"abbr\"]]\n",
    "pre['abbr'] = pre['abbr'].astype(str)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "\n",
    "plt.scatter(np.array(pre.share_voters_voted_trump),y=np.array(pre.share_population_with_high_school_degree),s=np.array(pre.hateCrimes)*4000,data=pre,alpha=1,color=\"r\")   \n",
    "plt.scatter(x=np.array(post.share_voters_voted_trump),y=np.array(post.share_population_with_high_school_degree),s=np.array(post.hateCrimes)*4000,data=post,alpha=0.4,color=\"y\")   \n",
    "\n",
    "plt.axis([0,0.75,0.785,0.925])\n",
    "ax.set_xlabel('Share voters voted for Trump')\n",
    "ax.set_ylabel('Share population with high school degree')\n",
    "ax.xaxis.label.set_size(12)\n",
    "ax.yaxis.label.set_size(12)\n",
    "\n",
    "z = randn(10)\n",
    "yellow_dot, = plt.plot(z, \"yo\", markersize=15)\n",
    "red_dot, =plt.plot(z, \"ro\", markersize=15)\n",
    "plt.legend([yellow_dot, red_dot],[\"post-election\",\"pre-election\"],loc=2)\n",
    "\n",
    "for i, txt in list(enumerate(np.array(pre.abbr))):\n",
    "    plt.annotate(np.array(txt), (np.array(pre.share_voters_voted_trump)[i]+0.01,np.array(pre.share_population_with_high_school_degree)[i]), size=12, weight='bold',rotation=10)\n",
    "\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the above plot, we observe that DC seems to be an outlier as it is away from the bulk of the data points. Specifically, the distance between its value  and other states' values in `share_voters_voted_trump` is significant. The large difference in radius between after and post-election circles indicates hate crimes increases rapidly right after the election. Some states only have red circles as they have missing values in hate crimes after the election. We will handle the missing values in the next section. \n",
    "\n",
    "To better understand the relationship between these covariates on `hateCrimes` for other states except DC , let's \"zoom in\" the plot:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If remove DC and re-do the plot (to zoom in)\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "\n",
    "plt.scatter(np.array(pre.share_voters_voted_trump),y=np.array(pre.share_population_with_high_school_degree),s=np.array(pre.hateCrimes)*7000,data=pre,alpha=1,color=\"r\")   \n",
    "plt.scatter(x=np.array(post.share_voters_voted_trump),y=np.array(post.share_population_with_high_school_degree),s=np.array(post.hateCrimes)*7000,data=post,alpha=.4,color=\"y\")   \n",
    "# add quantiles lines:\n",
    "plt.axhline(y=crimesL.share_population_with_high_school_degree[crimesL['abbr']!=\"DC\"].quantile(.5), color='r', linestyle=':')\n",
    "plt.axvline(x=crimesL.share_voters_voted_trump[crimesL['abbr']!=\"DC\"].quantile(.5), color='r', linestyle=':')\n",
    "\n",
    "plt.axis([0.3,0.75,0.785,0.925])\n",
    "ax.set_xlabel('Share voters voted for Trump')\n",
    "ax.set_ylabel('Share population with high school degree')\n",
    "ax.xaxis.label.set_size(12)\n",
    "ax.yaxis.label.set_size(12)\n",
    "\n",
    "z = randn(10)\n",
    "yellow_dot, = plt.plot(z, \"yo\", markersize=15)\n",
    "red_dot, =plt.plot(z, \"ro\", markersize=15)\n",
    "plt.legend([yellow_dot, red_dot],[\"post-election\",\"pre-election\"],loc=2)\n",
    "\n",
    "for i, txt in list(enumerate(np.array(pre.abbr))):\n",
    "    plt.annotate(np.array(txt), (np.array(pre.share_voters_voted_trump)[i]+0.005,np.array(pre.share_population_with_high_school_degree)[i]), size=10, weight='bold',rotation=10)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This \"zoom-in\" plot provides some further insight about the relationship between these variables. We add two additional lines which are the medians of `share_voters_voted_trump` (the red vertical line) and `share_population_with_high_school_degree` (the red horizontal line). \n",
    "A lot of states in the upper left corner have large values in hate crimes compared to other states (the circles are bigger). The upper left corner is where the share voters voted for Trump below the national average and the share population with high school degree is above the national average. The rises in hate crimes after the election seems to be significant for these states.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We further compute the difference in hate crimes after and before the election (for only states without missing values for now), where:\n",
    "\n",
    "$$diffCrimes = \\text{hate_crimes_per_100k_splc} - \\text{fbi_standardized_pre}$$\n",
    "\n",
    "Then, we calculate the correlation matrix and visualize it:\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimesCorr  = crimes.copy()\n",
    "crimesCorr  = crimesCorr.dropna()\n",
    "crimesCorr['diffCrimes'] = crimesCorr.hate_crimes_per_100k_splc - crimesCorr.fbi_standardized_pre\n",
    "\n",
    "topdropCorr = ['civilWar','abbr','avg_hatecrimes_per_100k_fbi','hate_crimes_per_100k_splc','fbi_standardized_pre']\n",
    "crimesCorr =  crimesCorr.drop(topdropCorr, axis=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Heat map for the correlation matrix\n",
    "corr = crimesCorr.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "f1 = sns.heatmap(corr, mask=mask, cmap=cmap, xticklabels=corr.columns.values,yticklabels=corr.columns.values, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the correlation plot, observe that some covariates are highly correlated and their correlation makes good sense demographically. For example, `share_non_citizen` and `share_population_in_metro_areas` have a strong positive correlation (correlation coefficient is $.77$) or `share_white_poverty` and `median_household_income` have a negative correlation of $-.83$. We want to pay a close attention to these correlation for the model building procedures. \n",
    "\n",
    "Similarly, we can examine the relationships between the covariates on the difference in hate crimes. The bubble plot we created early reflects the negative correlation between `share_voters_voted_trump` and the rises in hate crimes. With the correlation matrix, we have a concrete coefficient to measure their linear relationship. If we assume there are no other variables in the data, the linear coefficient between the two variables is $-.64$. Indeed, for states that have lower `share_voters_voted_trump`, the difference in hate crimes tend to increase. \n",
    "\n",
    "We also notice that the additional variable we added into the data, `percent_religious`, has an intermediate and negative correlation with `diffCrimes`(coefficient of $-0.36$) while its correlation with `share_population_with_high_school_degree` is the highest ($-0.58$) among all the covariates available in the data. We can futher explore their relationship in the scatter plot below:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "ax.grid(color='r', linestyle=':', linewidth=0.6)\n",
    "plt.axhline(y=crimesCorr['diffCrimes'].quantile(.5), color='b', linestyle='-')\n",
    "\n",
    "plt.scatter(x=crimesCorr['percent_religious'], y=crimesCorr['diffCrimes'], c=crimesCorr['share_population_with_high_school_degree'], \n",
    "            cmap='Spectral',s=250,edgecolors=\"k\",linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Religious percentage')\n",
    "ax.set_ylabel('Difference in crimes (Post-Pre)')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Share population with high school degree')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We produce a scatter plot between the difference in hate crimes  (y-axis) versus the religious percentage (x-axis). We color each data point using the value of share population with high school degree. The blue line is the 50% quantile of the difference in hate crimes. Since `percent_religious` has a negative correlated coefficient with `diffCrimes`, we observe the linear and negative slope in the above plot. Based on the colorbar, states with the highest values in `share_population_with_high_school_degree` and small percentage in religion, difference in hate crimes seems to be higher than the average.  Similarly, states with the smallest values in `share_population_with_high_school_degree` and the greatest values in `percent_religious`, the difference in hate crimes is below the average level. \n",
    "\n",
    "We also create a pair plot to see the relationship between each pair of the covariates in the data:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create the scatter plot:\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "g = sns.pairplot(crimesCorr,kind=\"reg\",size=3,diag_kind=\"kde\",diag_kws=dict(shade=True),palette=\"husl\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "One of the additional variables in this data is `civilWar`, which is coded as a categorical variable. We can examine the distribution in difference in hate crimes for each level in `civilWar` to see if civil war factors affect the hate crimes:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimesCivil  = crimes.copy()\n",
    "crimesCivil  = crimesCivil.dropna()\n",
    "crimesCivil['diffCrimes'] = crimesCivil.hate_crimes_per_100k_splc - crimesCivil.fbi_standardized_pre"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trace0 = go.Box(x=crimesCivil.diffCrimes[crimesCivil['civilWar']==\"union\"],name = 'Union',boxpoints='all',jitter=0.5,pointpos=-5,boxmean=True)\n",
    "trace1 = go.Box(x=crimesCivil.diffCrimes[crimesCivil['civilWar']==\"confe\"],name = 'Confederacy',boxpoints='all',jitter=0.5,pointpos=-5,boxmean=True)\n",
    "trace2 = go.Box(x=crimesCivil.diffCrimes[crimesCivil['civilWar']==\"none\"],name = 'None',boxpoints='all',jitter=0.5,pointpos=-5,boxmean=True)\n",
    "\n",
    "data = [trace0, trace1, trace2]\n",
    "py.iplot(data)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "K Means Clustering for the Hate Crime"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Make a copy\n",
    "crimes_deep_ref = deepcopy(crimes)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes2 = crimes_deep_ref\n",
    "type(crimes2)\n",
    "\n",
    "#Look at the columns\n",
    "crimes2.columns\n",
    "\n",
    "covariates = crimes2.drop(columns=['avg_hatecrimes_per_100k_fbi','hate_crimes_per_100k_splc', 'fbi_standardized_pre','abbr','civilWar'], axis=1)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(covariates)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(covariates)\n",
    "kmeans.labels_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Add the cluster column as a column\n",
    "\n",
    "crimes2[\"cluster_id\"] = kmeans.labels_\n",
    "#Impute using it by group_by cluster and finding the average for each one.\n",
    "\n",
    "#Will require adding old columns also.\n",
    "\n",
    "crimes2.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes2[[\"abbr\",\"cluster_id\"]].sort_values(by=[\"cluster_id\"])\n",
    "\n",
    "#Clusters visualized below."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Find the frequency for each type of cluster\n",
    "frq=collections.Counter(kmeans.labels_)\n",
    "print(frq)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#crimes2.info()\n",
    "\n",
    "#Group by cluster ID to get the means of the hate crimes per 100k SPLC for the different clusters.\n",
    "crimes2.groupby([\"cluster_id\"]).mean()[\"hate_crimes_per_100k_splc\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Assign the means of each cluster to the corresponding missing entry.\n",
    "\n",
    "#Find the states and cluster IDs with missing hate_crimes_per_100k_splc.\n",
    "crimes2[crimes2[\"hate_crimes_per_100k_splc\"].isnull()][\"cluster_id\"]\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Assign them the missing values.\n",
    "crimes2.loc[\"Hawaii\",\"hate_crimes_per_100k_splc\"] = .285794\n",
    "crimes2.loc[\"North Dakota\",\"hate_crimes_per_100k_splc\"] = .356692\n",
    "crimes2.loc[\"South Dakota\",\"hate_crimes_per_100k_splc\"] = .309873\n",
    "crimes2.loc[\"Wyoming\",\"hate_crimes_per_100k_splc\"] = .309873\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],[0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "     \n",
    "    #This colors it with rainbow colors.\n",
    "        colorscale = 'Rainbow',\n",
    "        autocolorscale = False,\n",
    "        \n",
    "    #This reverses the color scale\n",
    "        #reversescale = True,\n",
    "        locations = crimes['abbr'],\n",
    "        z = crimes2['cluster_id'].astype(int),\n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "             line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"Clustering based on covariates\")\n",
    "        )]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Clustering of United States by covariates',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)'),\n",
    "             )\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "py.iplot( fig, filename='covariate-cluster-plot' )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##Model\n",
    "\n",
    "1. found the difference of hate crimes before and after election\n",
    "2. what else did we do? - add to this section\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Find the difference of hate crime before and after election**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Make a copy for Crimes2\n",
    "crimes_diff = deepcopy(crimes2)\n",
    "#Hawaii doesn't have fbi_standardized_pre data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Find the cluster0 fbi_standardized_pre mean\n",
    "crimes_diff.groupby([\"cluster_id\"]).mean()[\"fbi_standardized_pre\"][0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Assign Hawaii the missing value by the group mean\n",
    "crimes_diff.loc[\"Hawaii\",\"fbi_standardized_pre\"] = 0.065741"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Find the difference between before and after election\n",
    "crimes_diff[\"difference\"] = crimes_diff[\"hate_crimes_per_100k_splc\"] - crimes_diff[\"fbi_standardized_pre\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Sort the difference \n",
    "crimes_diff[[\"abbr\",\"difference\"]].sort_values(by=[\"difference\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Plot the differences \n",
    "\n",
    "#scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],[0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "     \n",
    "    #This colors it with rainbow colors.\n",
    "        colorscale = 'Rainbow',\n",
    "        autocolorscale = False,\n",
    "        \n",
    "    #This reverses the color scale\n",
    "        #reversescale = True,\n",
    "        locations = crimes['abbr'],\n",
    "        z = crimes_diff['difference'].astype(float),\n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "             line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"Scaled after-before difference in crimes\")\n",
    "        )]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Difference in hate crimes after the election',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)'),\n",
    "             )\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "py.iplot( fig, filename='difference-crime-plot' )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###PCA\n",
    "\n",
    "This section of code below does Principal Component Analysis or PCA."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "covariates.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_std = StandardScaler().fit_transform(covariates)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca.fit(X_std)\n",
    "print(pca.components_)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Interpretting PCA loadings:\n",
    "- PCA1: (income, unemployed, metro, non citizen, gini, non white) vs (high school degreee, white povety, voted trump, percent religious, median age)\n",
    "- PCA2: (income, high school degree, non citizen, median age) vs(unemployed, metro, white poverty, gini, non white, voted trump, percent religous)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "#Call the function. Use only the 2 PCs.\n",
    "x_new = pca.fit_transform(X_std)\n",
    "\n",
    "myplot(x_new,np.transpose(pca.components_))\n",
    "#for i, txt in enumerate(covariates.index):\n",
    "#    plt.annotate(txt, (x_new[:,0][i],x_new[:,1][i]))\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(pca.explained_variance_)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca_all = PCA().fit(X_std)\n",
    "objects = ('PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10', 'PCA11')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos, pca_all.explained_variance_ratio_, align='center', alpha=0.75)\n",
    "plt.plot(np.cumsum(pca_all.explained_variance_ratio_), color=\"orange\", marker='o')\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained variance by different principal components')\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###Regression Tree on difference"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Define prediction indices\n",
    "#predict_indices = [1,2,3,4,5,6,7,8,11,12,13,16,17]\n",
    "\n",
    "#See if these work to get the columns we want\n",
    "#crimes_diff.columns[predict_indices]\n",
    "\n",
    "\n",
    "#Pick out the x and y variables for lasso usage\n",
    "#Remove cluster_id but perhaps add it later as a dummy variable.\n",
    "lasso_x = crimes_diff.drop(['hate_crimes_per_100k_splc','avg_hatecrimes_per_100k_fbi','civilWar','abbr','fbi_standardized_pre','cluster_id','difference'],axis = 1)\n",
    "\n",
    "#Make crimes_diff a pandas dataframe\n",
    "dummies_pd = pd.DataFrame(crimes_diff)\n",
    "\n",
    "#Convert cluster_id to a string\n",
    "dummies_pd['cluster_id'] = dummies_pd['cluster_id'].astype(str)\n",
    "\n",
    "#Get the dummmies of cluster_ID and civilWar\n",
    "dummies2 = pd.get_dummies(dummies_pd[['cluster_id','civilWar']])\n",
    "\n",
    "#Concatenate the dummies to the lasso_x frame\n",
    "lasso_x2 = pd.concat([pd.DataFrame(lasso_x), dummies2],axis=1)\n",
    "\n",
    "#Make the response\n",
    "lasso_y = crimes_diff['difference']\n",
    "\n",
    "dummies2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes_diff.columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "alphas = 10**np.linspace(-1,-5,100)*0.5\n",
    "#Fit Lasso to choose predictors?\n",
    "\n",
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(lasso_x2), lasso_y)\n",
    "    coefs.append(lasso.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(lasso_x2, lasso_y)\n",
    "\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(lasso_x2, lasso_y)\n",
    "\n",
    "pd.Series(lasso.coef_, index=lasso_x2.columns)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(lasso_x2, lasso_y)\n",
    "\n",
    "# Print coefficients with columns\n",
    "pd.Series(regr.coef_, index=lasso_x2.columns)\n",
    "\n",
    "X2 = sm.add_constant(lasso_x2)\n",
    "est = sm.OLS(lasso_y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crimes_diff.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tree_covariates = deepcopy(lasso_x2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# depth of tree: 5\n",
    "reg_depth5 = DecisionTreeRegressor(max_depth=5)\n",
    "reg_depth5_fit = reg_depth5.fit(tree_covariates,crimes_diff[\"difference\"])\n",
    "reg_depth5.feature_importances_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "importances1 = reg_depth5_fit.feature_importances_\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(tree_covariates.shape[1]), importances1,color=\"r\")\n",
    "plt.xticks(range(tree_covariates.shape[1]))\n",
    "plt.xlim([-1, tree_covariates.shape[1]])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When the depth of tree is 5. There are 8 features are not 0.\n",
    "\n",
    "Those features are:median_household_income,\n",
    "\n",
    " share_unemployed_seasonal,\n",
    "\n",
    "'share_population_in_metro_areas',\n",
    "\n",
    " 'share_population_with_high_school_degree',\n",
    " \n",
    " 'share_voters_voted_trump',\n",
    " \n",
    " 'percent_religious',\n",
    " \n",
    " 'medianAge', 'cluster_id_1'\n",
    "\n",
    "The most important feature is share_voters_voted_trump"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reg_depth5_fit = reg_depth5.fit(tree_covariates,crimes_diff[\"difference\"])\n",
    "reg_depth5_fit"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# depth of tree: 4\n",
    "#There are 6 features are significant different from 0\n",
    "reg_depth4 = DecisionTreeRegressor(max_depth=4)\n",
    "reg_depth4_fit = reg_depth4.fit(tree_covariates,crimes_diff[\"difference\"])\n",
    "reg_depth4.feature_importances_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "importances = reg_depth4.feature_importances_\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(tree_covariates.shape[1]), importances,color=\"r\")\n",
    "plt.xticks(range(tree_covariates.shape[1]))\n",
    "plt.xlim([-1, tree_covariates.shape[1]])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#The list for features\n",
    "list(tree_covariates)\n",
    "#5 Importan features: median_household_income,'share_unemployed_seasonal','share_population_with_high_school_degree','share_voters_voted_trump','percent_religious'\n",
    "#The most important feature is 'share_voters_voted_trump'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "export_graphviz(reg_depth4,out_file='tree.dot')\n",
    "#export_graphviz(reg_depth4)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "asdf = !cat tree.dot"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "\n",
    "# remove the display(...)\n",
    "\n",
    "g.Source(dot_graph)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "Source.from_file('tree.dot', format='dot')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dot = g.Digraph()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = lasso_x2\n",
    "df['diff'] = lasso_y"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "features"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "## try to fit a regular linear model:\n",
    "%%capture\n",
    "#gather features\n",
    "features = \"+\".join(lasso_x2.columns)\n",
    "\n",
    "# get y and X dataframes based on this regression:\n",
    "y, X = dmatrices('diff ~' + features, df, return_type='dataframe')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vif.round(1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "1. Also need to add to this section"
   ]
  }
 ]
}
